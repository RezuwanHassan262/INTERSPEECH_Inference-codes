{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T02:29:22.175858Z",
     "iopub.status.busy": "2023-07-18T02:29:22.175543Z",
     "iopub.status.idle": "2023-07-18T02:29:36.971462Z",
     "shell.execute_reply": "2023-07-18T02:29:36.970344Z",
     "shell.execute_reply.started": "2023-07-18T02:29:22.17583Z"
    },
    "id": "sIFOE9NL-JKb"
   },
   "source": [
    "<center><div class=\"alert alert-block alert-info\" style=\" line-height: 1.7em; font-family: Verdana;\">\n",
    "    <b style=\"font-size: 18px; color:green\"> &nbsp; Bengali.AI Speech Recognition : Our very first submission</b><br><br><b style=\"font-size: 18px; color:green\">HELLO EVERYONE!</b><br>\n",
    "</div></center>\n",
    "\n",
    "In this Notebook we'll try to submit our first submission by using a publicly available model.\n",
    "\n",
    "We'll be using this [BanglaASR](https://huggingface.co/bangla-speech-processing/BanglaASR) model. It is actually a finetuned [whisper](https://openai.com/research/whisper) model on rained [Bangla Mozilla Common Voice Dataset](https://arxiv.org/abs/2206.14053). It's reported performance is 4.58% on 7k validation set which seems pretty good to be a starting point. So let's start with that.\n",
    "\n",
    "I'll be trying to use the other publicly available models one by one and see how they perform. Then we'll get an idea from where we can start the training/finetuning on this huge dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjjIYZql-JKc"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T17:06:08.019536Z",
     "iopub.status.busy": "2024-03-09T17:06:08.018773Z",
     "iopub.status.idle": "2024-03-09T17:06:16.249476Z",
     "shell.execute_reply": "2024-03-09T17:06:16.248663Z",
     "shell.execute_reply.started": "2024-03-09T17:06:08.019504Z"
    },
    "id": "2rjBgVDo-JKe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4tHCpE1-JKe"
   },
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9HUPIxl-JKf"
   },
   "source": [
    "While submitting notebooks, you need to have internet disabled. So you can't download models from huggingface directly. One workaround is to download the models from huggingface, upload them to kaggle as datasets and then use them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T17:06:32.942798Z",
     "iopub.status.busy": "2024-03-09T17:06:32.942260Z",
     "iopub.status.idle": "2024-03-09T17:06:42.370496Z",
     "shell.execute_reply": "2024-03-09T17:06:42.369437Z",
     "shell.execute_reply.started": "2024-03-09T17:06:32.942767Z"
    },
    "id": "-Vg77ZmH-JKf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model_path = \"/kaggle/input/bangla-speech-processing-banglaasr/BanglaASR\"\n",
    "model_path = \"/kaggle/input/bangla-speech-processing-banglaasr\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_path)\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T9xFGSV-JKf"
   },
   "source": [
    "# Demo inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfB7twNA-JKg"
   },
   "source": [
    "We'll write an inference function where we'll give the path as input and it will return the transcription!\n",
    "\n",
    "First we'll need to convert the audio sampling rate to 16k since WhisperFeatureExtractor requires audios to be sampled at 16k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T17:06:42.372512Z",
     "iopub.status.busy": "2024-03-09T17:06:42.372218Z",
     "iopub.status.idle": "2024-03-09T17:06:42.379831Z",
     "shell.execute_reply": "2024-03-09T17:06:42.378656Z",
     "shell.execute_reply.started": "2024-03-09T17:06:42.372489Z"
    },
    "id": "cDOQI9Ho-JKg"
   },
   "outputs": [],
   "source": [
    "#This code is modified from the source code provided here https://huggingface.co/bangla-speech-processing/BanglaASR\n",
    "def inference_fn(path):\n",
    "    speech_array, sampling_rate = sf.read(mp3_path)\n",
    "    speech_array = librosa.resample(np.asarray(speech_array), orig_sr=sampling_rate, target_sr=16000)\n",
    "    input_features = feature_extractor(speech_array, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "    predicted_ids = model.generate(inputs=input_features.to(device))[0]\n",
    "    transcription = processor.decode(predicted_ids, skip_special_tokens=True)\n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5HdbrPe-JKg"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjBH6z-G-JKh",
    "outputId": "21e8643d-168d-4a85-fb15-689910b2dded"
   },
   "outputs": [],
   "source": [
    "mp3_path = \"/kaggle/input/local-stt-for-now/local_speech_for_now/tangail/rec_01_audio_0.wav\"\n",
    "print(f\"File name :\",mp3_path)\n",
    "AudioSegment.from_file(mp3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ODX0ajV-JKi",
    "outputId": "e5a2049e-bf73-4222-c0a7-08c009d9ece8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "test_path = \"/kaggle/input/local-stt-for-now/local_speech_for_now/tangail\"\n",
    "files = os.listdir(test_path)\n",
    "ids = []\n",
    "sentences = []\n",
    "for file in tqdm(files):\n",
    "    ids.append(file.split(\".\")[0])\n",
    "    mp3_path = os.path.join(test_path,file)\n",
    "    prediction = inference_fn(mp3_path)\n",
    "\n",
    "    #sanity check\n",
    "    if len(prediction)==0:\n",
    "        prediction = \"<>\"\n",
    "\n",
    "    sentences.append(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbaYaevp-RQF"
   },
   "outputs": [],
   "source": [
    "=================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO5R4xtQ-JKi",
    "outputId": "78f308fd-4890-4ae7-c5ba-f9963d673f0d"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"External_ID\":ids,\"Contents\":sentences})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRbtAi4G-JKi",
    "outputId": "c3a32514-fc3c-4727-8e3c-c61f60a27b7d"
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T19:37:53.637193Z",
     "iopub.status.busy": "2024-03-09T19:37:53.636627Z",
     "iopub.status.idle": "2024-03-09T19:37:53.673017Z",
     "shell.execute_reply": "2024-03-09T19:37:53.672340Z",
     "shell.execute_reply.started": "2024-03-09T19:37:53.637169Z"
    },
    "id": "OePHMoBr-JKi"
   },
   "outputs": [],
   "source": [
    "df1.to_csv(\"3.7 tangail-banglawhispe-inference.csv\",index=False)\n",
    "df1.to_excel(\"3.7 tangail-banglawhispe-inference.xlsx\",index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4557587,
     "sourceId": 7786721,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4563771,
     "sourceId": 7795474,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4570501,
     "sourceId": 7804929,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4570549,
     "sourceId": 7804994,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4570559,
     "sourceId": 7805010,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4570562,
     "sourceId": 7805014,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
